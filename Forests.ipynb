{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning and Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create and train a voting classifier w/ three diverse classifiers\n",
    "import numpy as np\n",
    "import os\n",
    "%matplotlib inline\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)), ('rf', RandomF...',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))],\n",
       "         flatten_transform=None, n_jobs=1, voting='soft', weights=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC(probability=True)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators = [('lr', log_clf),\n",
    "                  ('rf', rnd_clf),\n",
    "                  ('svc', svm_clf)],\n",
    "    voting = 'soft')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.904\n",
      "SVC 0.888\n",
      "VotingClassifier 0.912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# Let's look at each classifier's accuracy on the test set\n",
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and Pasting in Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train on an ensemble of 500 Decision Tree classifiers each trained on 100 training instances randomly\n",
    "# sampled from the training set with replacement\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators = 500,\n",
    "    max_samples = 100, bootstrap = True, n_jobs = -1)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.904"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-Bag Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8986666666666666"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set oob_score = True to request an automatic oob evaluation after training\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators = 500,\n",
    "    bootstrap = True, n_jobs = -1, oob_score = True)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So this is saying this BaggingClassifier is likely to achieve about 89.9% accuracy.\n",
    "# Let's see what happens:\n",
    "\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.45728643, 0.54271357],\n",
       "       [0.40697674, 0.59302326],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05527638, 0.94472362],\n",
       "       [0.39053254, 0.60946746],\n",
       "       [0.00980392, 0.99019608],\n",
       "       [0.99408284, 0.00591716],\n",
       "       [0.97368421, 0.02631579],\n",
       "       [0.75879397, 0.24120603],\n",
       "       [0.00574713, 0.99425287],\n",
       "       [0.77222222, 0.22777778],\n",
       "       [0.80446927, 0.19553073],\n",
       "       [0.97752809, 0.02247191],\n",
       "       [0.06077348, 0.93922652],\n",
       "       [0.00520833, 0.99479167],\n",
       "       [0.98369565, 0.01630435],\n",
       "       [0.89552239, 0.10447761],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01685393, 0.98314607],\n",
       "       [0.32571429, 0.67428571],\n",
       "       [0.92021277, 0.07978723],\n",
       "       [1.        , 0.        ],\n",
       "       [0.96650718, 0.03349282],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0052356 , 0.9947644 ],\n",
       "       [0.63541667, 0.36458333],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00540541, 0.99459459],\n",
       "       [0.        , 1.        ],\n",
       "       [0.1396648 , 0.8603352 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.32768362, 0.67231638],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.25625   , 0.74375   ],\n",
       "       [0.40804598, 0.59195402],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02617801, 0.97382199],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01129944, 0.98870056],\n",
       "       [0.9893617 , 0.0106383 ],\n",
       "       [0.9005848 , 0.0994152 ],\n",
       "       [0.98850575, 0.01149425],\n",
       "       [0.97222222, 0.02777778],\n",
       "       [0.        , 1.        ],\n",
       "       [0.06896552, 0.93103448],\n",
       "       [0.9947644 , 0.0052356 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01136364, 0.98863636],\n",
       "       [0.9947644 , 0.0052356 ],\n",
       "       [0.83246073, 0.16753927],\n",
       "       [0.4       , 0.6       ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.73446328, 0.26553672],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.79120879, 0.20879121],\n",
       "       [1.        , 0.        ],\n",
       "       [0.5989011 , 0.4010989 ],\n",
       "       [0.11111111, 0.88888889],\n",
       "       [0.69444444, 0.30555556],\n",
       "       [0.91370558, 0.08629442],\n",
       "       [0.        , 1.        ],\n",
       "       [0.25842697, 0.74157303],\n",
       "       [0.84974093, 0.15025907],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00518135, 0.99481865],\n",
       "       [0.06666667, 0.93333333],\n",
       "       [0.01058201, 0.98941799],\n",
       "       [0.28735632, 0.71264368],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.82258065, 0.17741935],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.25806452, 0.74193548],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.93975904, 0.06024096],\n",
       "       [0.78787879, 0.21212121],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.17021277, 0.82978723],\n",
       "       [0.64480874, 0.35519126],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05389222, 0.94610778],\n",
       "       [0.52459016, 0.47540984],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02824859, 0.97175141],\n",
       "       [1.        , 0.        ],\n",
       "       [0.29834254, 0.70165746],\n",
       "       [0.49746193, 0.50253807],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01554404, 0.98445596],\n",
       "       [0.99450549, 0.00549451],\n",
       "       [0.25287356, 0.74712644],\n",
       "       [0.89528796, 0.10471204],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.87096774, 0.12903226],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01075269, 0.98924731],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98453608, 0.01546392],\n",
       "       [0.99450549, 0.00549451],\n",
       "       [0.00555556, 0.99444444],\n",
       "       [0.93220339, 0.06779661],\n",
       "       [0.99415205, 0.00584795],\n",
       "       [0.02857143, 0.97142857],\n",
       "       [0.21578947, 0.78421053],\n",
       "       [0.95121951, 0.04878049],\n",
       "       [0.32446809, 0.67553191],\n",
       "       [0.99456522, 0.00543478],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.68604651, 0.31395349],\n",
       "       [0.34328358, 0.65671642],\n",
       "       [0.32222222, 0.67777778],\n",
       "       [0.88601036, 0.11398964],\n",
       "       [0.93717277, 0.06282723],\n",
       "       [0.07920792, 0.92079208],\n",
       "       [0.78333333, 0.21666667],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01851852, 0.98148148],\n",
       "       [0.98930481, 0.01069519],\n",
       "       [0.99418605, 0.00581395],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0052356 , 0.9947644 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01020408, 0.98979592],\n",
       "       [0.0060241 , 0.9939759 ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.94054054, 0.05945946],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.40686275, 0.59313725],\n",
       "       [0.28191489, 0.71808511],\n",
       "       [0.0106383 , 0.9893617 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.34337349, 0.65662651],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99408284, 0.00591716],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00531915, 0.99468085],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99489796, 0.00510204],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00966184, 0.99033816],\n",
       "       [0.64130435, 0.35869565],\n",
       "       [0.9010989 , 0.0989011 ],\n",
       "       [0.00561798, 0.99438202],\n",
       "       [0.99428571, 0.00571429],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.07361963, 0.92638037],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01183432, 0.98816568],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.03465347, 0.96534653],\n",
       "       [0.99468085, 0.00531915],\n",
       "       [0.91160221, 0.08839779],\n",
       "       [0.69791667, 0.30208333],\n",
       "       [0.54098361, 0.45901639],\n",
       "       [0.        , 1.        ],\n",
       "       [0.15591398, 0.84408602],\n",
       "       [1.        , 0.        ],\n",
       "       [0.95238095, 0.04761905],\n",
       "       [0.98888889, 0.01111111],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00552486, 0.99447514],\n",
       "       [0.        , 1.        ],\n",
       "       [0.41578947, 0.58421053],\n",
       "       [0.88709677, 0.11290323],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99401198, 0.00598802],\n",
       "       [0.03351955, 0.96648045],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98876404, 0.01123596],\n",
       "       [0.        , 1.        ],\n",
       "       [0.24719101, 0.75280899],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98469388, 0.01530612],\n",
       "       [0.76878613, 0.23121387],\n",
       "       [0.98907104, 0.01092896],\n",
       "       [0.        , 1.        ],\n",
       "       [0.06565657, 0.93434343],\n",
       "       [0.99431818, 0.00568182],\n",
       "       [0.04040404, 0.95959596],\n",
       "       [0.        , 1.        ],\n",
       "       [0.06741573, 0.93258427],\n",
       "       [1.        , 0.        ],\n",
       "       [0.77192982, 0.22807018],\n",
       "       [0.        , 1.        ],\n",
       "       [0.84771574, 0.15228426],\n",
       "       [0.98484848, 0.01515152],\n",
       "       [0.15196078, 0.84803922],\n",
       "       [0.22051282, 0.77948718],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.24390244, 0.75609756],\n",
       "       [0.95789474, 0.04210526],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.505     , 0.495     ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.08522727, 0.91477273],\n",
       "       [0.09139785, 0.90860215],\n",
       "       [0.97619048, 0.02380952],\n",
       "       [0.01104972, 0.98895028],\n",
       "       [1.        , 0.        ],\n",
       "       [0.35195531, 0.64804469],\n",
       "       [0.07291667, 0.92708333],\n",
       "       [0.54166667, 0.45833333],\n",
       "       [0.55882353, 0.44117647],\n",
       "       [0.01058201, 0.98941799],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.57653061, 0.42346939],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.2311828 , 0.7688172 ],\n",
       "       [0.7638191 , 0.2361809 ],\n",
       "       [0.06214689, 0.93785311],\n",
       "       [1.        , 0.        ],\n",
       "       [0.80337079, 0.19662921],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01136364, 0.98863636],\n",
       "       [0.06024096, 0.93975904],\n",
       "       [0.01522843, 0.98477157],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.89130435, 0.10869565],\n",
       "       [0.17159763, 0.82840237],\n",
       "       [0.97191011, 0.02808989],\n",
       "       [0.00540541, 0.99459459],\n",
       "       [0.60946746, 0.39053254],\n",
       "       [0.10326087, 0.89673913],\n",
       "       [0.98888889, 0.01111111],\n",
       "       [0.8071066 , 0.1928934 ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.95977011, 0.04022989],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.26737968, 0.73262032],\n",
       "       [0.99465241, 0.00534759],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00568182, 0.99431818],\n",
       "       [0.83919598, 0.16080402],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.76436782, 0.23563218],\n",
       "       [0.95108696, 0.04891304],\n",
       "       [1.        , 0.        ],\n",
       "       [0.70899471, 0.29100529],\n",
       "       [0.50819672, 0.49180328],\n",
       "       [0.        , 1.        ],\n",
       "       [0.87894737, 0.12105263],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.87790698, 0.12209302],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.76086957, 0.23913043],\n",
       "       [0.11764706, 0.88235294],\n",
       "       [0.42134831, 0.57865169],\n",
       "       [0.25      , 0.75      ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.89940828, 0.10059172],\n",
       "       [0.8373494 , 0.1626506 ],\n",
       "       [0.00531915, 0.99468085],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02030457, 0.97969543],\n",
       "       [0.97206704, 0.02793296],\n",
       "       [0.95597484, 0.04402516],\n",
       "       [1.        , 0.        ],\n",
       "       [0.52040816, 0.47959184],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98963731, 0.01036269],\n",
       "       [0.01587302, 0.98412698],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.96202532, 0.03797468],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05681818, 0.94318182],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99418605, 0.00581395],\n",
       "       [0.01807229, 0.98192771],\n",
       "       [1.        , 0.        ],\n",
       "       [0.14285714, 0.85714286],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01546392, 0.98453608],\n",
       "       [0.        , 1.        ],\n",
       "       [0.36046512, 0.63953488],\n",
       "       [0.06077348, 0.93922652],\n",
       "       [0.2311828 , 0.7688172 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99421965, 0.00578035],\n",
       "       [0.18681319, 0.81318681],\n",
       "       [0.98787879, 0.01212121],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.94972067, 0.05027933],\n",
       "       [0.28654971, 0.71345029],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00505051, 0.99494949],\n",
       "       [0.99418605, 0.00581395],\n",
       "       [0.00584795, 0.99415205],\n",
       "       [0.01098901, 0.98901099],\n",
       "       [0.99479167, 0.00520833],\n",
       "       [1.        , 0.        ],\n",
       "       [0.03424658, 0.96575342],\n",
       "       [0.63783784, 0.36216216]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train a Random Forest classifier with 500 trees (each limited to maximum 16 nodes), using all available CPU cores\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators = 500, max_leaf_nodes = 16, n_jobs = -1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This BaggingClassifier is roughly the same as the above cell\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(splitter = 'random', max_leaf_nodes = 16),\n",
    "    n_estimators = 500, max_samples = 1.0, bootstrap = True, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.10525185076034302\n",
      "sepal width (cm) 0.025126722068359905\n",
      "petal length (cm) 0.4236915209767608\n",
      "petal width (cm) 0.44592990619453604\n"
     ]
    }
   ],
   "source": [
    "# Train a RandomForestClassifier on the iris dataset and output each feature's importance\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators = 500, n_jobs = -1)\n",
    "rnd_clf.fit(iris['data'], iris['target'])\n",
    "for name, score in zip(iris['feature_names'], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "          learning_rate=0.5, n_estimators=200, random_state=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train an AdaBoost classifier based on 200 Decision Stumps\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth = 1), n_estimators = 200,\n",
    "    algorithm = \"SAMME.R\", learning_rate = 0.5)\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit a DecisionTreeRegressor to the training set\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth = 2)\n",
    "tree_reg1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a second DecisionTreeRegressor on the residual errors made by the predicotr\n",
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth = 2)\n",
    "tree_reg2.fit(X, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a third regressor on the residual errors made by the second predictor\n",
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth = 2)\n",
    "tree_reg3.fit(X, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This gives us an ensemble, make predictions by adding up the predictions of all three\n",
    "\n",
    "\n",
    "X_new = np.array([[0.8]])\n",
    "\n",
    "\n",
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75026781])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=1.0, loss='ls', max_depth=2, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=3, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This creates the same ensemble as above:\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth = 2, n_estimators = 3, learning_rate = 1.0)\n",
    "gbrt.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=2, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=84, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a GBRT ensemble with 120 trees, measure the validation error at each stage of training to find the\n",
    "# optimal number of trees, and train another GBRT ensemble using the optimal number of trees\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth = 2, n_estimators = 120)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "errors = [mean_squared_error(y_val, y_pred)\n",
    "          for y_pred in gbrt.staged_predict(X_val)]\n",
    "bst_n_estimators = np.argmin(errors)\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth = 2, n_estimators = bst_n_estimators)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This will stop training when the validation error doesn't improve for 5 iterations in a row\n",
    "gbrt = GradientBoostingRegressor(max_depth = 2, warm_start = True)\n",
    "\n",
    "min_val_error = float('inf')\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            break   #early stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercises\n",
    "## #8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create train, test, val sets\n",
    "X_train, X_test, X_val = X[:50000], X[50000:60000], X[60000:]\n",
    "y_train, y_test, y_val = y[:50000], y[50000:60000], y[60000:]\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffle the training set\n",
    "import numpy as np\n",
    "\n",
    "shuffle_index = np.random.permutation(50000)\n",
    "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train various models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier(max_depth = 2, random_state = 0)\n",
    "rf_clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=10,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n",
       "           oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "ext_clf = ExtraTreesClassifier(n_estimators=200, max_depth=None, min_samples_split=10, random_state=0)\n",
    "ext_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=5, cache_size=200, class_weight=None, coef0=1,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='poly',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "svm_clf = SVC(kernel = 'poly', degree = 3, coef0=1, C=5)\n",
    "svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest F1: 0.6248312541060128\n",
      "Extra-trees F1: 0.9074312955719895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM F1: 0.9213049678300277\n"
     ]
    }
   ],
   "source": [
    "# Check how they perform individually on the validation set\n",
    "from sklearn.metrics import f1_score\n",
    "rf_pred = rf_clf.predict(X_val)\n",
    "rf_f1 = f1_score(rf_pred, y_val, average = 'weighted')\n",
    "print(\"Random Forest F1:\", rf_f1)\n",
    "\n",
    "ext_pred = ext_clf.predict(X_val)\n",
    "ext_f1 = f1_score(ext_pred, y_val, average = 'weighted')\n",
    "print(\"Extra-trees F1:\", ext_f1)\n",
    "\n",
    "svm_pred = poly_kernel_svm_clf.predict(X_val)\n",
    "svm_f1 = f1_score(svm_pred, y_val, average = 'weighted')\n",
    "print(\"SVM F1:\", svm_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('rfc', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weig... max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))]))],\n",
       "         flatten_transform=None, n_jobs=1, voting='hard', weights=None)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now create the ensemble\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "ens_clf1 = RandomForestClassifier(max_depth = 2, random_state = 0)\n",
    "ens_clf2 = ExtraTreesClassifier(n_estimators=200, max_depth=None, min_samples_split=10, random_state=0)\n",
    "ens_clf3 = SVC(kernel = 'poly', degree = 3, coef0=1, C=5)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators = [('rfc', ens_clf1), ('etc', ens_clf2), ('svm', ens_clf3)],\n",
    "    voting = 'soft')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9083653753767718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "vote_pred = voting_clf.predict(X_val)\n",
    "vote_f1 = f1_score(vote_pred, y_val, average = 'weighted')\n",
    "print(vote_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting F1:  0.3143734009881159\n",
      "Random Forest F1:  0.0\n",
      "Extra Trees F1:  0.32647258107039284\n",
      "SVM F1:  0.3724525436408978\n"
     ]
    }
   ],
   "source": [
    "# Check on the test set\n",
    "vote_pred = voting_clf.predict(X_test)\n",
    "print(\"Voting F1: \", f1_score(vote_pred, y_test, average = 'weighted'))\n",
    "rf_pred = rf_clf.predict(X_test)\n",
    "print(\"Random Forest F1: \", f1_score(rf_pred, y_test, average = 'weighted'))\n",
    "ext_pred = ext_clf.predict(X_test)\n",
    "print(\"Extra Trees F1: \", f1_score(ext_pred, y_test, average = 'weighted'))\n",
    "svm_pred = svm_clf.predict(X_test)\n",
    "print(\"SVM F1: \", f1_score(svm_pred, y_test, average = 'weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 6., 1., ..., 7., 3., 3.])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8., 8., 8., ..., 9., 9., 9.])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = list(zip(rf_clf.predict(X_val), ext_clf.predict(X_val), svm_clf.predict(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 4.0, 4.0)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors[9000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blend_clf = SVC(kernel = 'poly', degree = 3, coef0=1, C=5)\n",
    "blend_clf.fit(vectors, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_vec = list(zip(rf_clf.predict(X_test), ext_clf.predict(X_test), svm_clf.predict(X_test)))\n",
    "test_pred = blend_clf.predict(test_vec)\n",
    "print(\"Blender F1: \", f1_score(test_pred, y_test, average = 'weighted'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
